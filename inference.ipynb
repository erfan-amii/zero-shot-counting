{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from torchvision.utils import save_image\n",
    "from config import cfg \n",
    "import util.misc as utils\n",
    "from loss import get_loss\n",
    "from FSC147_dataset import build_dataset, batch_collate_fn, random_aug_boxes, get_image_classes\n",
    "from engine import evaluate, train_one_epoch, visualization \n",
    "from models import build_model\n",
    "from torch.distributions import uniform, normal\n",
    "from models.regressor import get_regressor\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vae import FeatsVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read only one image\n",
    "sample_image_address = \"/Users/erfan/Documents/GitHub/zero-shot-counting/SWATCam44_20220617_001912_273_SWATCam44_20220617_001703858.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def select_feats_vae_imgnet(vae_feature, patches, model):\n",
    "    patch_feature = model.backbone(patches)\n",
    "    tmp_patch = model.EPF_extractor.avgpool(patch_feature).flatten(1) \n",
    "    dist = (tmp_patch - vae_feature)**2\n",
    "    dist = dist.sum(1)\n",
    "    return dist.argsort()[:10]\n",
    "\n",
    "def select_feats_vae(vae_feature, patches, model):\n",
    "    patch_feature = model.backbone(patches)\n",
    "    tmp_patch = model.EPF_extractor.avgpool(patch_feature).flatten(1) \n",
    "    dist = (tmp_patch - vae_feature)**2\n",
    "    dist = dist.sum(1)\n",
    "    return dist.argsort()[:100]\n",
    "\n",
    "def prepare_data(img_path, anno):\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    gtcount = len(anno['points'])\n",
    "    boxes = np.array(anno['box_examples_coordinates'])\n",
    "    boxes = random_aug_boxes(boxes, img.size[1], img.size[0])\n",
    "    query_transform = transforms.Compose([\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    patches = []\n",
    "    scale_embedding = [] \n",
    "    scale_number = 20\n",
    "    for box in boxes:\n",
    "        x1, y1 = box[0].astype(np.int32)\n",
    "        x2, y2 = box[2].astype(np.int32)\n",
    "        #x1,y1,x2,y2 = np.array(box).astype(np.int32)\n",
    "        patch = img.crop((x1, y1, x2, y2))\n",
    "        patches.append(query_transform(patch))\n",
    "        scale = (x2 - x1) / w * 0.5 + (y2 -y1) / h * 0.5\n",
    "        scale = scale // (0.5 / scale_number)\n",
    "        scale = scale if scale < scale_number - 1 else scale_number - 1\n",
    "        scale_embedding.append(0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    main_transform = transforms.Compose([transforms.Resize(size=384), \\\n",
    "                   transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "    img = main_transform(img)\n",
    "    return img.unsqueeze(0), patches.unsqueeze(0), torch.tensor(scale_embedding).unsqueeze(0).to(torch.int64), gtcount, boxes\n",
    "\n",
    "def get_vae_embedding(attr_np):\n",
    "    feats_vae = FeatsVAE(1024, 512).cuda()\n",
    "    feats_vae.load_state_dict(torch.load('feats_vae.pth'))\n",
    "    z_dist = normal.Normal(0, 1)\n",
    "    ind_count = 500\n",
    "    attr = torch.from_numpy(attr_np.astype(np.float32)).cuda()\n",
    "    attr = attr.repeat(ind_count, 1)\n",
    "    Z = z_dist.sample((ind_count, 512)).cuda()\n",
    "    concat_feats = torch.cat((Z, attr), dim=1)\n",
    "    feats = feats_vae.model(concat_feats)\n",
    "    feats = feats_vae.relu(feats_vae.bn1(feats))\n",
    "    return feats.cpu().mean(0)\n",
    "\n",
    "def extract_corr_map(args):\n",
    "    #print(args)\n",
    "    device = torch.device(cfg.TRAIN.device)\n",
    "    # fix the seed for reproducibility\n",
    "    # seed = cfg.TRAIN.seed\n",
    "    # torch.manual_seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    # random.seed(seed)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "   \n",
    "    regressor = get_regressor(cfg)\n",
    "    regressor.to(device)\n",
    "    regressor.eval()\n",
    "    regressor.load_state_dict(torch.load('regressor_model/regressor.pth'))    \n",
    " \n",
    "\n",
    "    # define dataset\n",
    "    # output_dir = Path(cfg.DIR.output_dir)\n",
    "    # cls_dict = get_image_classes('./FSC147_384_V2/ImageClasses_FSC147.txt')\n",
    "    # cls_list = np.array(list(cls_dict.values()))\n",
    "    # cls_list = sorted(np.unique(cls_list))\n",
    "    # vae_feats = np.load(os.path.join(output_dir, 'fsc_vae_feats_best.npy'), allow_pickle=True)\n",
    "    # checkpoint = torch.load(cfg.VAL.resume, map_location='cpu')\n",
    "    # model_imgnet = copy.deepcopy(model)\n",
    "    # model.load_state_dict(checkpoint['model'])\n",
    "    # mae = 0\n",
    "    # mse = 0\n",
    "    # nae = 0\n",
    "    # sre = 0\n",
    "    # count_idx = 0\n",
    "    # loss_avg = 0\n",
    "    # errs_all = []\n",
    "    #with open('FSC_multiclass_val_test_All_Boxes.pkl', 'rb') as pickle_file:\n",
    "    #  annos = pickle.load(pickle_file)\n",
    "    # with open('FSC147_384_V2/annotation_FSC147_384.json', 'rb') as pickle_file:\n",
    "    #   annos = json.load(pickle_file)\n",
    "    # count_item = 0\n",
    "    # tmp_list = []\n",
    "    # train_list = [name.split('\\t') for name in open('FSC147_384_V2/test.txt').read().splitlines()]\n",
    "    # for idxx, k in enumerate(train_list):\n",
    "    img, patches1, scale_embedding, gtcount, boxes = prepare_data('./FSC147_384_V2/images_384_VarV2/%s'%k[0], annos[k[0]])\n",
    "\n",
    "    img = img.to(device)\n",
    "    scale_embedding = scale_embedding.to(device)\n",
    "    patches = patches1.to(device)\n",
    "        with torch.no_grad():\n",
    "          ###################\n",
    "          ori_features1 = model.backbone(img)\n",
    "          ori_features = model.input_proj(ori_features1)\n",
    "          ###################\n",
    "          ###################\n",
    "          img = F.interpolate(img, [384,384])\n",
    "          features = model.backbone(img)\n",
    "          features = model.input_proj(features)\n",
    "          patches = patches.flatten(0, 1)\n",
    "          cls = cls_dict[k[0]]\n",
    "          label = cls_list.index(cls)\n",
    "          patch_feature = model.backbone(patches) # obtain feature maps for exemplar patches\n",
    "          vae_feature = vae_feats[label]\n",
    "          #vae_sel_idx = select_feats_vae_imgnet((vae_feature.mean(0)).to(device), patches, model_imgnet)\n",
    "          vae_sel_idx = select_feats_vae_imgnet(torch.from_numpy(vae_feature).to(device), patches, model_imgnet)\n",
    "          patch_feature2 = model.EPF_extractor(patch_feature[vae_sel_idx], scale_embedding[:, vae_sel_idx])\n",
    "          bs, batch_num_patches = scale_embedding.shape\n",
    "          refined_feature, patch_feature2 = model.refiner(ori_features, patch_feature2)\n",
    "          counting_feature, corr_map = model.matcher(refined_feature, patch_feature2)\n",
    "          bs, c, h, w = refined_feature.shape\n",
    "          feats_all = []\n",
    "          if True:\n",
    "            for m_idx in range(patch_feature2.shape[0]):\n",
    "              counting_feature, corr_map = model.matcher(features, patch_feature2[[m_idx]])\n",
    "              feats_all.append(counting_feature)\n",
    "            counting_feature = torch.stack(feats_all).squeeze(1)\n",
    "            scores = regressor(counting_feature)\n",
    "            sel_idx = scores.argsort(0)[:3]\n",
    "            patch_feature3 = patch_feature2[sel_idx[:,0]]\n",
    "            counting_feature, corr_map = model.matcher(refined_feature, patch_feature3)\n",
    "          density_map = model.counter(counting_feature)\n",
    "          error = torch.abs(density_map.sum() - gtcount).item()\n",
    "          errs_all.append(error)\n",
    "          print('%s: gt: %d, err: %d'%(k[0], int(gtcount), int(error)))\n",
    "          count_item += 1\n",
    "          mae += error\n",
    "          mse += error ** 2\n",
    "          nae += error / gtcount\n",
    "          sre += error ** 2 / gtcount\n",
    "    mae = mae / count_item\n",
    "    mse = mse / count_item\n",
    "    nae = nae / count_item\n",
    "    sre = sre / count_item\n",
    "    mse = mse ** 0.5\n",
    "    sre = sre ** 0.5\n",
    "    print('MAE %.2f, MSE %.2f, NAE %.2f, SRE %.2f \\n'%(mae, mse, nae, sre))\n",
    "           \n",
    "\n",
    "\n",
    "def extract_corr_map_customized(cfg):\n",
    "  device = torch.device(cfg.TRAIN.device)\n",
    "  model = build_model(cfg)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  regressor = get_regressor(cfg)\n",
    "  regressor.to(device)\n",
    "  regressor.eval()\n",
    "  regressor.load_state_dict(torch.load('regressor_model/regressor.pth'))\n",
    "  img = Image.open(sample_image_address)\n",
    "  w, h = img.size\n",
    "  main_transform = transforms.Compose([transforms.Resize(size=384), \\\n",
    "                   transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "  img = main_transform(img).unsqueeze(0)\n",
    "  img = img.to(device)\n",
    "  with torch.no_grad():\n",
    "    ori_features1 = model.backbone(img)\n",
    "    ori_features = model.input_proj(ori_features1)\n",
    "    ###################\n",
    "    ###################\n",
    "    img = F.interpolate(img, [384,384])\n",
    "    features = model.backbone(img)\n",
    "    features = model.input_proj(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--cfg FILE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9011 --control=9009 --hb=9008 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"ded61ce2-82e3-438d-8845-b18d7d78ed5f\" --shell=9010 --transport=\"tcp\" --iopub=9012 --f=/Users/erfan/Library/Jupyter/runtime/kernel-v2-11775E0dpEY3Ouyua.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erfan/miniconda3/envs/cvpr23_zeroshot_venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Class Agnostic Object Counting in PyTorch\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cfg\",\n",
    "        default=\"config/bmnet+_fsc147.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "        type=str,\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg.merge_from_file(args.cfg)\n",
    "    #cfg.merge_from_list(args.opts)\n",
    "    \n",
    "    cfg.DIR.output_dir = os.path.join(cfg.DIR.snapshot, cfg.DIR.exp)\n",
    "    if not os.path.exists(cfg.DIR.output_dir):\n",
    "        os.mkdir(cfg.DIR.output_dir)    \n",
    "\n",
    "    cfg.TRAIN.resume = os.path.join(cfg.DIR.output_dir, cfg.TRAIN.resume)\n",
    "    cfg.VAL.resume = os.path.join(cfg.DIR.output_dir, cfg.VAL.resume)\n",
    "\n",
    "    with open(os.path.join(cfg.DIR.output_dir, 'config.yaml'), 'w') as f:\n",
    "        f.write(\"{}\".format(cfg))\n",
    "\n",
    "    extract_corr_map(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr23_zeroshot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
