{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from torchvision.utils import save_image\n",
    "from config import cfg \n",
    "import util.misc as utils\n",
    "from loss import get_loss\n",
    "from FSC147_dataset import build_dataset, batch_collate_fn, random_aug_boxes, get_image_classes\n",
    "from engine import evaluate, train_one_epoch, visualization \n",
    "from models import build_model\n",
    "from torch.distributions import uniform, normal\n",
    "from models.regressor import get_regressor\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vae import FeatsVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read only one image\n",
    "sample_image_address = \"/Users/erfan/Documents/GitHub/zero-shot-counting/SWATCam44_20220617_001912_273_SWATCam44_20220617_001703858.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pickle5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb#W3sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvae\u001b[39;00m \u001b[39mimport\u001b[39;00m FeatsVAE\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle5\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_feats_vae_imgnet\u001b[39m(vae_feature, patches, model):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/erfan/Documents/GitHub/zero-shot-counting/inference.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     patch_feature \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbackbone(patches)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pickle5'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Training code for bilinear similarity network (BMNet and BMNet+)\n",
    "# --cfg: path for configuration file\n",
    "# ------------------------------------------------------------------------\n",
    "import argparse\n",
    "import datetime\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "import os \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from torchvision.utils import save_image\n",
    "from config import cfg \n",
    "import util.misc as utils\n",
    "from loss import get_loss\n",
    "from FSC147_dataset import build_dataset, batch_collate_fn, random_aug_boxes, get_image_classes\n",
    "from engine import evaluate, train_one_epoch, visualization \n",
    "from models import build_model\n",
    "from torch.distributions import uniform, normal\n",
    "from models.regressor import get_regressor\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from models.vae import FeatsVAE\n",
    "\n",
    "\n",
    "def select_feats_vae_imgnet(vae_feature, patches, model):\n",
    "    patch_feature = model.backbone(patches)\n",
    "    tmp_patch = model.EPF_extractor.avgpool(patch_feature).flatten(1) \n",
    "    dist = (tmp_patch - vae_feature)**2\n",
    "    dist = dist.sum(1)\n",
    "    return dist.argsort()[:10]\n",
    "\n",
    "def select_feats_vae(vae_feature, patches, model):\n",
    "    patch_feature = model.backbone(patches)\n",
    "    tmp_patch = model.EPF_extractor.avgpool(patch_feature).flatten(1) \n",
    "    dist = (tmp_patch - vae_feature)**2\n",
    "    dist = dist.sum(1)\n",
    "    return dist.argsort()[:100]\n",
    "\n",
    "def prepare_data(img_path, anno):\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    gtcount = len(anno['points'])\n",
    "    boxes = np.array(anno['box_examples_coordinates'])\n",
    "    boxes = random_aug_boxes(boxes, img.size[1], img.size[0])\n",
    "    query_transform = transforms.Compose([\n",
    "            transforms.Resize((128,128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    patches = []\n",
    "    scale_embedding = [] \n",
    "    scale_number = 20\n",
    "    for box in boxes:\n",
    "        x1, y1 = box[0].astype(np.int32)\n",
    "        x2, y2 = box[2].astype(np.int32)\n",
    "        #x1,y1,x2,y2 = np.array(box).astype(np.int32)\n",
    "        patch = img.crop((x1, y1, x2, y2))\n",
    "        patches.append(query_transform(patch))\n",
    "        scale = (x2 - x1) / w * 0.5 + (y2 -y1) / h * 0.5\n",
    "        scale = scale // (0.5 / scale_number)\n",
    "        scale = scale if scale < scale_number - 1 else scale_number - 1\n",
    "        scale_embedding.append(0)\n",
    "    patches = torch.stack(patches, dim=0)\n",
    "    main_transform = transforms.Compose([transforms.Resize(size=384), \\\n",
    "                   transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
    "    img = main_transform(img)\n",
    "    return img.unsqueeze(0), patches.unsqueeze(0), torch.tensor(scale_embedding).unsqueeze(0).to(torch.int64), gtcount, boxes\n",
    "\n",
    "def get_vae_embedding(attr_np):\n",
    "    feats_vae = FeatsVAE(1024, 512).cuda()\n",
    "    feats_vae.load_state_dict(torch.load('feats_vae.pth'))\n",
    "    z_dist = normal.Normal(0, 1)\n",
    "    ind_count = 500\n",
    "    attr = torch.from_numpy(attr_np.astype(np.float32)).cuda()\n",
    "    attr = attr.repeat(ind_count, 1)\n",
    "    Z = z_dist.sample((ind_count, 512)).cuda()\n",
    "    concat_feats = torch.cat((Z, attr), dim=1)\n",
    "    feats = feats_vae.model(concat_feats)\n",
    "    feats = feats_vae.relu(feats_vae.bn1(feats))\n",
    "    return feats.cpu().mean(0)\n",
    "\n",
    "def extract_corr_map(args):\n",
    "    #print(args)\n",
    "    device = torch.device(cfg.TRAIN.device)\n",
    "    # fix the seed for reproducibility\n",
    "    seed = cfg.TRAIN.seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "   \n",
    "    regressor = get_regressor(cfg)\n",
    "    regressor.to(device)\n",
    "    regressor.eval()\n",
    "    regressor.load_state_dict(torch.load('regressor_model/regressor.pth'))    \n",
    " \n",
    "\n",
    "    # define dataset\n",
    "    output_dir = Path(cfg.DIR.output_dir)\n",
    "    cls_dict = get_image_classes('./FSC147_384_V2/ImageClasses_FSC147.txt')\n",
    "    cls_list = np.array(list(cls_dict.values()))\n",
    "    cls_list = sorted(np.unique(cls_list))\n",
    "    vae_feats = np.load(os.path.join(output_dir, 'fsc_vae_feats_best.npy'), allow_pickle=True)\n",
    "    checkpoint = torch.load(cfg.VAL.resume, map_location='cpu')\n",
    "    model_imgnet = copy.deepcopy(model)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    mae = 0\n",
    "    mse = 0\n",
    "    nae = 0\n",
    "    sre = 0\n",
    "    count_idx = 0\n",
    "    loss_avg = 0\n",
    "    errs_all = []\n",
    "    #with open('FSC_multiclass_val_test_All_Boxes.pkl', 'rb') as pickle_file:\n",
    "    #  annos = pickle.load(pickle_file)\n",
    "    with open('FSC147_384_V2/annotation_FSC147_384.json', 'rb') as pickle_file:\n",
    "      annos = json.load(pickle_file)\n",
    "    count_item = 0\n",
    "    tmp_list = []\n",
    "    train_list = [name.split('\\t') for name in open('FSC147_384_V2/test.txt').read().splitlines()]\n",
    "    for idxx, k in enumerate(train_list):\n",
    "        img, patches1, scale_embedding, gtcount, boxes = prepare_data('./FSC147_384_V2/images_384_VarV2/%s'%k[0], annos[k[0]])\n",
    "        img = img.to(device)\n",
    "        scale_embedding = scale_embedding.to(device)\n",
    "        patches = patches1.to(device)\n",
    "        with torch.no_grad():\n",
    "          ###################\n",
    "          ori_features1 = model.backbone(img)\n",
    "          ori_features = model.input_proj(ori_features1)\n",
    "          ###################\n",
    "          ###################\n",
    "          img = F.interpolate(img, [384,384])\n",
    "          features = model.backbone(img)\n",
    "          features = model.input_proj(features)\n",
    "          patches = patches.flatten(0, 1)\n",
    "          cls = cls_dict[k[0]]\n",
    "          label = cls_list.index(cls)\n",
    "          patch_feature = model.backbone(patches) # obtain feature maps for exemplar patches\n",
    "          vae_feature = vae_feats[label]\n",
    "          #vae_sel_idx = select_feats_vae_imgnet((vae_feature.mean(0)).to(device), patches, model_imgnet)\n",
    "          vae_sel_idx = select_feats_vae_imgnet(torch.from_numpy(vae_feature).to(device), patches, model_imgnet)\n",
    "          patch_feature2 = model.EPF_extractor(patch_feature[vae_sel_idx], scale_embedding[:, vae_sel_idx])\n",
    "          bs, batch_num_patches = scale_embedding.shape\n",
    "          refined_feature, patch_feature2 = model.refiner(ori_features, patch_feature2)\n",
    "          counting_feature, corr_map = model.matcher(refined_feature, patch_feature2)\n",
    "          bs, c, h, w = refined_feature.shape\n",
    "          feats_all = []\n",
    "          if True:\n",
    "            for m_idx in range(patch_feature2.shape[0]):\n",
    "              counting_feature, corr_map = model.matcher(features, patch_feature2[[m_idx]])\n",
    "              feats_all.append(counting_feature)\n",
    "            counting_feature = torch.stack(feats_all).squeeze(1)\n",
    "            scores = regressor(counting_feature)\n",
    "            sel_idx = scores.argsort(0)[:3]\n",
    "            patch_feature3 = patch_feature2[sel_idx[:,0]]\n",
    "            counting_feature, corr_map = model.matcher(refined_feature, patch_feature3)\n",
    "          density_map = model.counter(counting_feature)\n",
    "          error = torch.abs(density_map.sum() - gtcount).item()\n",
    "          errs_all.append(error)\n",
    "          print('%s: gt: %d, err: %d'%(k[0], int(gtcount), int(error)))\n",
    "          count_item += 1\n",
    "          mae += error\n",
    "          mse += error ** 2\n",
    "          nae += error / gtcount\n",
    "          sre += error ** 2 / gtcount\n",
    "    mae = mae / count_item\n",
    "    mse = mse / count_item\n",
    "    nae = nae / count_item\n",
    "    sre = sre / count_item\n",
    "    mse = mse ** 0.5\n",
    "    sre = sre ** 0.5\n",
    "    print('MAE %.2f, MSE %.2f, NAE %.2f, SRE %.2f \\n'%(mae, mse, nae, sre))\n",
    "           \n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Class Agnostic Object Counting in PyTorch\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--cfg\",\n",
    "        default=\"config/bmnet+_fsc147.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "        type=str,\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    cfg.merge_from_file(args.cfg)\n",
    "    #cfg.merge_from_list(args.opts)\n",
    "    \n",
    "    cfg.DIR.output_dir = os.path.join(cfg.DIR.snapshot, cfg.DIR.exp)\n",
    "    if not os.path.exists(cfg.DIR.output_dir):\n",
    "        os.mkdir(cfg.DIR.output_dir)    \n",
    "\n",
    "    cfg.TRAIN.resume = os.path.join(cfg.DIR.output_dir, cfg.TRAIN.resume)\n",
    "    cfg.VAL.resume = os.path.join(cfg.DIR.output_dir, cfg.VAL.resume)\n",
    "\n",
    "    with open(os.path.join(cfg.DIR.output_dir, 'config.yaml'), 'w') as f:\n",
    "        f.write(\"{}\".format(cfg))\n",
    "\n",
    "    extract_corr_map(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvpr23_zeroshot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
